/* This file is auto-generated by `just build-migrations`; DO NOT EDIT */
export const getMigrations = (schemaName: string) => ({
  "0000000000_migrations.sql": `CREATE SCHEMA IF NOT EXISTS pgconductor;

CREATE TABLE pgconductor.schema_migrations (
  version INT PRIMARY KEY,
  name TEXT NOT NULL,
  applied_at TIMESTAMP DEFAULT NOW(),
  breaking boolean default false
);`,
  "0000000001_setup.sql": `-- todo: partitioning, sharding
-- todo:
-- - throttling (limit, period, key),
-- - concurrency (limit, key),
-- - rateLimit (limit, period, key),
-- - debounce (period, key) -> via invoke
-- throttling, concurrency and rateLimit: key and seconds - fetch and group by - USE SLOTS similar to https://planetscale.com/blog/the-slotted-counter-pattern
-- maybe: cel as a postgres extension -> no, rust binary with cdc
-- batch processing via array payloads?
-- check what extensions are available on install and store it on a settings table, e.g. pg_jsonschema, pg_partman, ...
-- we need to fetch workflow config on fetch executions and add executions anyways...

-- assert required extensions are installed
DO $$
BEGIN
  IF NOT EXISTS (SELECT 1 FROM pg_extension WHERE extname = 'pg_partman') THEN
    RAISE EXCEPTION 'Extension pg_partman is not installed. Run: CREATE EXTENSION pg_partman;';
  END IF;

  IF NOT EXISTS (SELECT 1 FROM pg_extension WHERE extname = 'uuid-ossp') THEN
    RAISE EXCEPTION 'Extension uuid-ossp is not installed. Run: CREATE EXTENSION "uuid-ossp";';
  END IF;
END $$;

create schema if not exists pgconductor;

-- Returns either the actual current timestamp or a fake one if
-- the session sets \`pgconductor.fake_now\`. This lets tests control time.
create function pgconductor.current_time ()
  returns timestamptz
  language plpgsql
  volatile
as $$
declare
  v_fake text;
begin
  v_fake := current_setting('pgconductor.fake_now', true);
  if v_fake is not null and length(trim(v_fake)) > 0 then
    return v_fake::timestamptz;
  end if;

  return clock_timestamp();
end;
$$;

CREATE TABLE pgconductor.settings (
    key text primary key,
    value jsonb not null
);

-- add pro features here later
-- store package and migration version
-- enable live migrations
-- we could also store the heartbeat of workers here but i think refreshing the locks is cleaner

-- todo: how to make sure we handle sudden worker crashes? we could leverage steps to also update locked_at periodically
-- or have a separate heartbeat mechanism per execution
-- and if a execution has been locked for too long, we can assume the worker crashed and make it available again
-- DANGER: this could lead to multiple workers working on the same execution if the heartbeat interval is too long
-- separate heartbeat mechanism is safer than leveraging steps because steps might not be executed frequently enough and we do
-- not want this burden on the user
-- we could even just put the heartbeat on the workers table and have a background process that checks for stale locks
-- this should be fine too and it means we do not have to join the workers table when fetching executions
-- just refresh locked_at?

-- add breaking marker in migrations

CREATE TABLE pgconductor.workers (
    id uuid primary key,
    last_heartbeat_at TIMESTAMPTZ NOT NULL DEFAULT NOW(),
    version text,
    migration_number integer,
    -- if true, the worker should shut down gracefully
    shutdown_signal boolean default false not null
);

-- simple heartbeat functions that updates last_heartbeat_at and returns a boolean indicating whether the worker should shut down
CREATE OR REPLACE FUNCTION pgconductor.worker_heartbeat(v_worker_id uuid)
 RETURNS boolean
 LANGUAGE sql
 VOLATILE
 SET search_path TO ''
AS $function$
update pgconductor.workers
set last_heartbeat_at = pgconductor.current_time()
where id = v_worker_id
returning shutdown_signal;
$function$
;

-- todo: sweep function that updates shutdown_signal for other workers into an advisory lock e.g. when deploying a new version
-- todo: stale worker cleanup function that removes workers and unlocks their executions if last_heartbeat_at is too old

-- TODO: add settings here
CREATE TABLE pgconductor.workflows (
    key text primary key,
    -- we could validate execution payloads on insert if pg jsonchema extension is available
    input_schema jsonb default '{}'::jsonb not null,

    -- retry settings
    max_attempts integer default 3 not null,
    retry_exponential_backoff boolean default true not null,
    retry_delay_seconds integer default 60 not null,
    max_retry_delay_seconds integer,

    visibility_timeout interval default interval '5 minutes',

    -- workflow can be executed only within certain time windows
    -- e.g. business hours, weekends, nights, ...
    -- we will stop the execution of executions outside of these time windows at step boundaries
    -- TODO: we should adjust run_at based on these windows when inserting / rescheduling executions
    window_start timetz,
    window_end timetz,
    CONSTRAINT "windows" CHECK (
        (window_start IS NULL AND window_end IS NULL) OR
        (
            window_start IS NOT NULL AND
            window_end IS NOT NULL AND
            window_start != window_end
        )
    )
);

CREATE TABLE pgconductor.executions (
    id uuid primary key,
    workflow_key text not null,
    key text,
    created_at timestamptz default pgconductor.current_time() not null,
    failed_at timestamptz,
    completed_at timestamptz,
    payload jsonb default '{}'::jsonb not null,
    run_at timestamptz default pgconductor.current_time() not null,
    locked_at timestamptz,
    locked_by uuid,
    attempts integer default 0 not null,
    last_error text,
    priority integer default 0 not null
) PARTITION BY LIST (workflow_key) with (fillfactor=70);
-- it would be more efficient to query across all workflows? this is why graphile worker does not partition at all
-- we will partition - it should scale better

-- for "once" workflows / data migrations?
CREATE TABLE pgconductor.user_migrations (
    key uuid primary key,
    started_at TIMESTAMPTZ NOT NULL DEFAULT NOW()
);

CREATE TABLE pgconductor.failed_executions (LIKE executions INCLUDING ALL) PARTITION BY RANGE (failed_at);

SELECT partman.create_parent(
    p_parent_table => 'pgconductor.failed_executions',
    p_control => 'failed_at',
    p_type => 'native',
    p_interval => '1 day',
    p_premake => 7,
    p_start_partition => current_date::timestamptz,
    p_inherit_fk => true
);

CREATE TABLE pgconductor.completed_executions (LIKE executions INCLUDING ALL) PARTITION BY RANGE (completed_at);

SELECT partman.create_parent(
    p_parent_table => 'pgconductor.completed_executions',
    p_control => 'completed_at',
    p_type => 'native',
    p_interval => '1 day',
    p_premake => 7,
    p_start_partition => current_date::timestamptz,
    p_inherit_fk => true
);

UPDATE partman.part_config
SET
    retention = '7 days',
    retention_keep_table = false,
    retention_keep_index = false
WHERE parent_table IN (
    'pgconductor.failed_executions',
    'pgconductor.completed_executions'
);

CREATE TABLE pgconductor.steps (
    id bigint primary key generated always as identity,
    key text not null,
    execution_id bigint not null,
    result jsonb default '{}'::jsonb not null,
    created_at timestamptz default pgconductor.current_time() not null
);

-- TODO: use this table to manage concurrency, throttling and rate limiting
-- manage them when workflow is being updated/created
CREATE TABLE pgconductor.slots (
    key text primary key,
    workflow_key text not null,
    locked_at timestamptz,
    locked_by uuid
);

CREATE TABLE pgconductor.schedule (
    name text REFERENCES {{schema}}.workflows ON DELETE CASCADE,
    key text not null,
    cron text not null,
    timezone text,
    data jsonb,
    PRIMARY KEY (name, key)
);

-- utility function to generate a uuidv7 even for older postgres versions.
create function pgconductor.portable_uuidv7 ()
  returns uuid
  language plpgsql
  volatile
as $$
declare
  v_server_num integer := current_setting('server_version_num')::int;
  ts_ms bigint;
  b bytea;
  rnd bytea;
  i int;
begin
  if v_server_num >= 180000 then
    return uuidv7 ();
  end if;
  ts_ms := floor(extract(epoch from absurd.current_time()) * 1000)::bigint;
  rnd := uuid_send(uuid_generate_v4 ());
  b := repeat(E'\\000', 16)::bytea;
  for i in 0..5 loop
    b := set_byte(b, i, ((ts_ms >> ((5 - i) * 8)) & 255)::int);
  end loop;
  for i in 6..15 loop
    b := set_byte(b, i, get_byte(rnd, i));
  end loop;
  b := set_byte(b, 6, ((get_byte(b, 6) & 15) | (7 << 4)));
  b := set_byte(b, 8, ((get_byte(b, 8) & 63) | 128));
  return encode(b, 'hex')::uuid;
end;
$$;

CREATE OR REPLACE FUNCTION pgconductor.get_executions(v_workflow_key text, v_worker_id uuid, v_batch_size integer default 100)
 RETURNS TABLE(id uuid, payload jsonb)
 LANGUAGE sql
 STABLE
 SET search_path TO ''
AS $function$
with workflow as (
  select window_start,
         window_end,
         visibility_timeout
  from pgconductor.workflows
  where key = v_workflow_key
),

-- Only proceed if weâ€™re inside the active window (or no window defined)
allowed as (
  select 1
  from workflow w
  where
    (w.window_start is null and w.window_end is null)
    or (
      pgconductor.current_time()::timetz >= w.window_start
      and pgconductor.current_time()::timetz < w.window_end
    )
),

e as (
  select e.id
  from pgconductor.executions e
  join workflow w on true
  where exists (select 1 from allowed)
    and e.attempts < w.max_attempts
    and (
      e.locked_at is null
      or e.locked_at + (w.visibility_timeout * interval '1 second') <= pgconductor.current_time()
    )
    and e.run_at <= pgconductor.current_time()
    and e.workflow_key = v_workflow_key
  order by e.priority asc, e.run_at asc
  limit v_batch_size
  for update skip locked
)

update pgconductor.executions
set
  attempts = executions.attempts + 1,
  locked_by = v_worker_id,
  locked_at = pgconductor.current_time()
from e
where executions.id = e.id
returning executions.id, executions.payload;
$function$
;

-- copy successful executions into a separate table for reporting
-- the separate table is partitioned by time for easier cleanup
CREATE OR REPLACE FUNCTION pgconductor.complete_executions(v_execution_ids uuid[])
 RETURNS void
 LANGUAGE sql
 STABLE
 SET search_path TO ''
AS $function$
with deleted as (
  delete from pgconductor.executions
  where id = any(v_execution_ids)
  returning *
)
insert into pgconductor.completed_executions
select
  id,
  workflow_key,
  key,
  created_at,
  failed_at,
  pgconductor.current_time() as completed_at,
  payload,
  run_at,
  locked_at,
  locked_by,
  last_heartbeat_at,
  visibility_timeout,
  attempts,
  last_error,
  priority
from deleted;
$function$
;

-- when worker is shutting down or we are outside of the window now, we want to release locked executions
CREATE OR REPLACE FUNCTION pgconductor.release_executions(v_execution_ids uuid[])
 RETURNS void
 LANGUAGE sql
 STABLE
 SET search_path TO ''
AS $function$
update pgconductor.executions e
set
  -- remove one attempt as this attempt was not completed but did not fail either
  attempts = greatest(e.attempts - 1, 0),
  locked_by = null,
  locked_at = null
where e.id = any(v_execution_ids);
$function$
;

-- when max attempts are reached, copy failed executions into a separate table for reporting
-- the separate table is partitioned by time for easier cleanup
CREATE OR REPLACE FUNCTION pgconductor.fail_executions(v_execution_ids uuid[])
 RETURNS TABLE(id uuid, payload jsonb)
 LANGUAGE sql
 STABLE
 SET search_path TO ''
AS $function$
-- First, select workflow settings once
with workflow as (
  select retry_exponential_backoff,
         retry_delay_seconds,
         max_retry_delay_seconds,
         max_attempts
  from pgconductor.workflows
  where key = v_workflow_key
),

-- Delete executions that have reached max attempts
deleted_executions as (
  delete from pgconductor.executions e
  using workflow w
  where e.id = any(v_execution_ids)
    and e.attempts >= w.max_attempts
  returning e.*
),

-- Retry executions that can still be retried
retried_executions as (
  update pgconductor.executions e
  set
    failed_at = pgconductor.current_time(),
    last_error = null,
    run_at = case
      when w.retry_exponential_backoff then
        greatest(pgconductor.current_time(), e.run_at)
        + least(
            exp(least(e.attempts, 10)) * interval '1 second',
            coalesce(w.max_retry_delay_seconds * interval '1 second', interval '1 hour')
          )
      else
        greatest(pgconductor.current_time(), e.run_at)
        + (w.retry_delay_seconds * interval '1 second')
    end,
    attempts = e.attempts + 1,
    locked_by = null,
    locked_at = null
  from workflow w
  where e.id = any(v_execution_ids)
    and e.attempts < w.max_attempts
  returning e.*
)

insert into pgconductor.failed_executions
select
  id,
  workflow_key,
  key,
  created_at,
  pgconductor.current_time() as failed_at,
  completed_at,
  payload,
  run_at,
  locked_at,
  locked_by,
  last_heartbeat_at,
  visibility_timeout,
  attempts,
  last_error,
  priority
from deleted_executions;
$function$
;

create type pgconductor.execution_spec as (
    workflow_key text,
    payload jsonb,
    run_at timestamptz,
    key text,
    priority integer
);


CREATE OR REPLACE FUNCTION pgconductor.invoke(
    workflow_key text,
    payload jsonb default null,
    run_at timestamptz default null,
    key text default null,
    priority integer default null
)
 RETURNS uuid
 LANGUAGE sql
 VOLATILE
 SET search_path TO ''
AS $function$
select id from pgconductor.invoke(array[
    (workflow_key, payload, run_at, key, priority)::pgconductor.execution_spec
])
$function$
;

CREATE OR REPLACE FUNCTION pgconductor.invoke(
    specs pgconductor.execution_spec[]
)
 RETURNS TABLE(id uuid)
 LANGUAGE plpgsql
 VOLATILE
 SET search_path TO ''
AS $function$
begin
  -- Ensure any locked executions have their key cleared - in the case of locked
  -- existing execution create a new execution instead as it must have already started
  -- executing (i.e. it's world state is out of date, and the fact invoke
  -- has been called again implies there's new information that needs to be
  -- acted upon).
  update pgconductor.executions as e
  set
    key = null,
    attempts = e.max_attempts,
    updated_at = pgconductor.current_time()
  from unnest(specs) spec
  where spec.key is not null
  and e.key = spec.key
  and is_available is not true;

  return query insert into pgconductor.executions as e (
    id,
    workflow_key,
    payload,
    run_at,
    key,
    priority
  )
    select
      pgconductor.portable_uuidv7(),
      spec.workflow_key,
      coalesce(spec.payload, '{}'::json),
      coalesce(spec.run_at, pgconductor.current_time()),
      spec.key,
      coalesce(spec.priority, 0)
    from unnest(specs) spec
  on conflict (key) do update set
    workflow_key = excluded.workflow_key,
    payload =
      case
      when json_typeof(executions.payload) = 'array' and json_typeof(excluded.payload) = 'array' then
        (executions.payload::jsonb || excluded.payload::jsonb)::json
      else
        excluded.payload
      end,
    max_attempts = excluded.max_attempts,
    run_at = excluded.run_at,
    priority = excluded.priority,
    -- always reset error/retry state
    attempts = 0,
    last_error = null
  where executions.locked_at is null
  returning e.id;
end;
$function$
;`,
});
